.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_tutorials_topi_intro_topi.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_topi_intro_topi.py:


Introduction to TOPI
====================
**Author**: `Ehsan M. Kermani <https://github.com/ehsanmok>`_

This is an introductory tutorial to TVM Operator Inventory (TOPI).
TOPI provides numpy-style generic operations and schedules with higher abstractions than TVM.
In this tutorial, we will see how TOPI can save us from writing boilerplates code in TVM.


.. code-block:: default

    from __future__ import absolute_import, print_function

    import tvm
    from tvm import te
    import topi
    import numpy as np







Basic example
-------------
Let's revisit the sum of rows operation (equivalent to :code:`B = numpy.sum(A, axis=1)`') \
To compute the sum of rows of a two dimensional TVM tensor A, we should
specify the symbolic operation as well as schedule as follows



.. code-block:: default

    n = te.var("n")
    m = te.var("m")
    A = te.placeholder((n, m), name='A')
    k = te.reduce_axis((0, m), "k")
    B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k), name="B")
    s = te.create_schedule(B.op)







and to examine the IR code in human readable format, we can do



.. code-block:: default

    print(tvm.lower(s, [A], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    // attr [B] storage_scope = "global"
    allocate B[float32 * n]
    produce B {
      for (i, 0, n) {
        B[i] = 0f
        for (k, 0, m) {
          B[i] = (B[i] + A[((i*stride) + (k*stride))])
        }
      }
    }




However, for such a common operation we had to define the reduce axis ourselves as well as explicit computation with
:code:`te.compute`. Imagine for more complicated operations how much details we need to provide.
Fortunately, we can replace those two lines with simple :code:`topi.sum` much like :code:`numpy.sum`



.. code-block:: default

    C = topi.sum(A, axis=1)
    ts = te.create_schedule(C.op)
    print(tvm.lower(ts, [A], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    // attr [A_red] storage_scope = "global"
    allocate A_red[float32 * n]
    produce A_red {
      for (ax0, 0, n) {
        A_red[ax0] = 0f
        for (k1, 0, m) {
          A_red[ax0] = (A_red[ax0] + A[((ax0*stride) + (k1*stride))])
        }
      }
    }




Numpy-style operator overloading
--------------------------------
We can add two tensors using :code:`topi.broadcast_add` that have correct (broadcastable with specific) shapes.
Even shorter, TOPI provides operator overloading for such common operations. For example,



.. code-block:: default

    x, y = 100, 10
    a = te.placeholder((x, y, y), name="a")
    b = te.placeholder((y, y), name="b")
    c = a + b  # same as topi.broadcast_add
    d = a * b  # same as topi.broadcast_mul







Overloaded with the same syntax, TOPI handles broadcasting a primitive (`int`, `float`) to a tensor :code:`d - 3.14`.

Generic schedules and fusing operations
---------------------------------------
Up to now, we have seen an example of how TOPI can save us from writing explicit computations in lower level API.
But it doesn't stop here. Still we did the scheduling as before. TOPI also provides higher level
scheduling recipes depending on a given context. For example, for CUDA,
we can schedule the following series of operations ending with :code:`topi.sum` using only
:code:`topi.generic.schedule_reduce`



.. code-block:: default

    e = topi.elemwise_sum([c, d])
    f = e / 2.0
    g = topi.sum(f)
    with tvm.target.cuda():
        sg = topi.cuda.schedule_reduce(g)
        print(tvm.lower(sg, [a, b], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    // attr [T_divide_red] storage_scope = "global"
    allocate T_divide_red[float32 * 1]
    produce T_divide_red {
      // attr [iter_var(threadIdx.x, range(min=0, ext=1024), threadIdx.x)] thread_extent = 1024
      // attr [T_divide_red.rf] storage_scope = "local"
      allocate T_divide_red.rf[float32 * 1]
      // attr [reduce_temp0] storage_scope = "local"
      allocate reduce_temp0[float32 * 1]
      produce T_divide_red.rf {
        T_divide_red.rf[0] = 0f
        for (k0.k1.fused.k2.fused.outer, 0, 10) {
          if (likely((((((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x) < 10000) && (((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x) < 10000)) && (((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x) < 10000)))) {
            T_divide_red.rf[0] = (T_divide_red.rf[0] + (((a[((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x)] + b[floormod(((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x), 100)]) + (a[((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x)]*b[floormod(((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x), 100)]))*0.5f))
          }
        }
      }
      // attr [comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f])] reduce_scope = reinterpret((uint64)0)
      tvm_thread_allreduce((uint32)1, T_divide_red.rf[0], (bool)1, reduce_temp0, threadIdx.x)
      if ((threadIdx.x == 0)) {
        T_divide_red[0] = reduce_temp0[0]
      }
    }




As you can see, scheduled stages of computation have been accumulated and we can examine them by



.. code-block:: default

    print(sg.stages)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [stage(a, 0x103c1c890), stage(b, 0x105cf1aa0), stage(T_add, 0xf30add10), stage(T_multiply, 0xf6cfb5a0), stage(T_elemwise_sum, 0xf30aa600), stage(T_divide, 0x106619d90), stage(T_divide_red.rf, 0x105cf6550), stage(T_divide_red, 0x126faa7a0)]



We can test the correctness by comparing with :code:`numpy` result as follows



.. code-block:: default

    func = tvm.build(sg, [a, b, g], 'cuda')
    ctx = tvm.gpu(0)
    a_np = np.random.uniform(size=(x, y, y)).astype(a.dtype)
    b_np = np.random.uniform(size=(y, y)).astype(b.dtype)
    g_np = np.sum(np.add(a_np + b_np, a_np * b_np) / 2.0)
    a_nd = tvm.nd.array(a_np, ctx)
    b_nd = tvm.nd.array(b_np, ctx)
    g_nd = tvm.nd.array(np.zeros(g_np.shape, dtype=g_np.dtype), ctx)
    func(a_nd, b_nd, g_nd)
    tvm.testing.assert_allclose(g_nd.asnumpy(), g_np, rtol=1e-5)







TOPI also provides common neural nets operations such as _softmax_ with optimized schedule



.. code-block:: default

    tarray = te.placeholder((512, 512), name="tarray")
    softmax_topi = topi.nn.softmax(tarray)
    with tvm.target.create("cuda"):
        sst = topi.cuda.schedule_softmax(softmax_topi)
        print(tvm.lower(sst, [tarray], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    // attr [T_softmax_maxelem] storage_scope = "global"
    allocate T_softmax_maxelem[float32 * 512]
    // attr [T_softmax_exp] storage_scope = "global"
    allocate T_softmax_exp[float32 * 262144]
    produce T_softmax_maxelem {
      // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 512
      T_softmax_maxelem[blockIdx.x] = -3.40282e+38f
      for (k, 0, 512) {
        T_softmax_maxelem[blockIdx.x] = max(T_softmax_maxelem[blockIdx.x], tarray[((blockIdx.x*512) + k)])
      }
    }
    produce T_softmax_exp {
      // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 512
      for (i1, 0, 512) {
        T_softmax_exp[((blockIdx.x*512) + i1)] = exp((tarray[((blockIdx.x*512) + i1)] - T_softmax_maxelem[blockIdx.x]))
      }
    }
    produce T_softmax_expsum {
      // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 512
      // attr [T_softmax_expsum.rf] storage_scope = "local"
      allocate T_softmax_expsum.rf[float32 * 1]
      // attr [reduce_temp0] storage_scope = "local"
      allocate reduce_temp0[float32 * 1]
      // attr [iter_var(threadIdx.x, range(min=0, ext=64), threadIdx.x)] thread_extent = 64
      produce T_softmax_expsum.rf {
        T_softmax_expsum.rf[0] = 0f
        for (k.outer, 0, 8) {
          T_softmax_expsum.rf[0] = (T_softmax_expsum.rf[0] + T_softmax_exp[(((blockIdx.x*512) + (k.outer*64)) + threadIdx.x)])
        }
      }
      // attr [comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f])] reduce_scope = reinterpret((uint64)0)
      tvm_thread_allreduce((uint32)1, T_softmax_expsum.rf[0], (bool)1, reduce_temp0, threadIdx.x)
      if ((threadIdx.x == 0)) {
        T_softmax_maxelem[blockIdx.x] = reduce_temp0[0]
      }
    }
    produce T_softmax_norm {
      // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 512
      // attr [iter_var(threadIdx.x, range(min=0, ext=64), threadIdx.x)] thread_extent = 64
      for (i1.inner, 0, 8) {
        T_softmax_exp[(((blockIdx.x*512) + (threadIdx.x*8)) + i1.inner)] = (T_softmax_exp[(((blockIdx.x*512) + (threadIdx.x*8)) + i1.inner)]/T_softmax_maxelem[blockIdx.x])
      }
    }




Fusing convolutions
-------------------
We can fuse :code:`topi.nn.conv2d` and :code:`topi.nn.relu` together.

.. note::

   TOPI functions are all generic functions. They have different implementations
   for different backends to optimize for performance.
   For each backend, it is necessary to call them under a target scope for both
   compute declaration and schedule. TVM will choose the right function to call with
   the target information.


.. code-block:: default


    data = te.placeholder((1, 3, 224, 224))
    kernel = te.placeholder((10, 3, 5, 5))

    with tvm.target.create("cuda"):
        conv = topi.cuda.conv2d_nchw(data, kernel, 1, 2, 1)
        out = topi.nn.relu(conv)
        sconv = topi.cuda.schedule_conv2d_nchw([out])
        print(tvm.lower(sconv, [data, kernel], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    // attr [compute] storage_scope = "global"
    allocate compute[float32 * 501760]
    produce compute {
      // attr [iter_var(blockIdx.z, , blockIdx.z)] thread_extent = 5
      // attr [compute] storage_scope = "local"
      allocate compute[float32 * 14]
      // attr [pad_temp.shared] storage_scope = "shared"
      allocate pad_temp.shared[float32 * 112]
      // attr [placeholder.shared] storage_scope = "shared"
      allocate placeholder.shared[float32 * 2]
      // attr [iter_var(blockIdx.y, , blockIdx.y)] thread_extent = 224
      // attr [iter_var(blockIdx.x, , blockIdx.x)] thread_extent = 2
      // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
      // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
      // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
      produce compute {
        compute[0] = 0f
        compute[2] = 0f
        compute[4] = 0f
        compute[6] = 0f
        compute[8] = 0f
        compute[10] = 0f
        compute[12] = 0f
        compute[1] = 0f
        compute[3] = 0f
        compute[5] = 0f
        compute[7] = 0f
        compute[9] = 0f
        compute[11] = 0f
        compute[13] = 0f
        for (rc.outer, 0, 3) {
          for (ry.outer, 0, 5) {
            produce pad_temp.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              pad_temp.shared[(threadIdx.x*7)] = tvm_if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (2 <= ((blockIdx.x*112) + (threadIdx.x*7)))), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 450)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 1)] = tvm_if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (1 <= ((blockIdx.x*112) + (threadIdx.x*7)))), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 449)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 2)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 448)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 3)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 447)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 4)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 446)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 5)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 445)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 6)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 444)], 0f)
            }
            produce placeholder.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              if (likely((threadIdx.x < 2))) {
                if (likely((threadIdx.x < 2))) {
                  if (likely((threadIdx.x < 2))) {
                    if (likely((threadIdx.x < 2))) {
                      if (likely((threadIdx.x < 2))) {
                        if (likely((threadIdx.x < 2))) {
                          if (likely((((blockIdx.z*2) + threadIdx.x) < 10))) {
                            placeholder.shared[threadIdx.x] = placeholder[((((blockIdx.z*150) + (threadIdx.x*75)) + (rc.outer*25)) + (ry.outer*5))]
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
            compute[0] = (compute[0] + (pad_temp.shared[threadIdx.x]*placeholder.shared[0]))
            compute[2] = (compute[2] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[0]))
            compute[4] = (compute[4] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[0]))
            compute[6] = (compute[6] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[0]))
            compute[8] = (compute[8] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[0]))
            compute[10] = (compute[10] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[0]))
            compute[12] = (compute[12] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[0]))
            compute[1] = (compute[1] + (pad_temp.shared[threadIdx.x]*placeholder.shared[1]))
            compute[3] = (compute[3] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[1]))
            compute[5] = (compute[5] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[1]))
            compute[7] = (compute[7] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[1]))
            compute[9] = (compute[9] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[1]))
            compute[11] = (compute[11] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[1]))
            compute[13] = (compute[13] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[1]))
            produce pad_temp.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              pad_temp.shared[(threadIdx.x*7)] = tvm_if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (1 <= ((blockIdx.x*112) + (threadIdx.x*7)))), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 449)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 1)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 448)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 2)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 447)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 3)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 446)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 4)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 445)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 5)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 444)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 6)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 443)], 0f)
            }
            produce placeholder.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              if (likely((threadIdx.x < 2))) {
                if (likely((threadIdx.x < 2))) {
                  if (likely((threadIdx.x < 2))) {
                    if (likely((threadIdx.x < 2))) {
                      if (likely((threadIdx.x < 2))) {
                        if (likely((threadIdx.x < 2))) {
                          if (likely((((blockIdx.z*2) + threadIdx.x) < 10))) {
                            placeholder.shared[threadIdx.x] = placeholder[(((((blockIdx.z*150) + (threadIdx.x*75)) + (rc.outer*25)) + (ry.outer*5)) + 1)]
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
            compute[0] = (compute[0] + (pad_temp.shared[threadIdx.x]*placeholder.shared[0]))
            compute[2] = (compute[2] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[0]))
            compute[4] = (compute[4] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[0]))
            compute[6] = (compute[6] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[0]))
            compute[8] = (compute[8] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[0]))
            compute[10] = (compute[10] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[0]))
            compute[12] = (compute[12] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[0]))
            compute[1] = (compute[1] + (pad_temp.shared[threadIdx.x]*placeholder.shared[1]))
            compute[3] = (compute[3] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[1]))
            compute[5] = (compute[5] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[1]))
            compute[7] = (compute[7] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[1]))
            compute[9] = (compute[9] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[1]))
            compute[11] = (compute[11] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[1]))
            compute[13] = (compute[13] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[1]))
            produce pad_temp.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              pad_temp.shared[(threadIdx.x*7)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 448)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 1)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 447)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 2)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 446)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 3)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 445)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 4)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 444)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 5)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 443)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 6)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 442)], 0f)
            }
            produce placeholder.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              if (likely((threadIdx.x < 2))) {
                if (likely((threadIdx.x < 2))) {
                  if (likely((threadIdx.x < 2))) {
                    if (likely((threadIdx.x < 2))) {
                      if (likely((threadIdx.x < 2))) {
                        if (likely((threadIdx.x < 2))) {
                          if (likely((((blockIdx.z*2) + threadIdx.x) < 10))) {
                            placeholder.shared[threadIdx.x] = placeholder[(((((blockIdx.z*150) + (threadIdx.x*75)) + (rc.outer*25)) + (ry.outer*5)) + 2)]
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
            compute[0] = (compute[0] + (pad_temp.shared[threadIdx.x]*placeholder.shared[0]))
            compute[2] = (compute[2] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[0]))
            compute[4] = (compute[4] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[0]))
            compute[6] = (compute[6] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[0]))
            compute[8] = (compute[8] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[0]))
            compute[10] = (compute[10] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[0]))
            compute[12] = (compute[12] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[0]))
            compute[1] = (compute[1] + (pad_temp.shared[threadIdx.x]*placeholder.shared[1]))
            compute[3] = (compute[3] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[1]))
            compute[5] = (compute[5] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[1]))
            compute[7] = (compute[7] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[1]))
            compute[9] = (compute[9] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[1]))
            compute[11] = (compute[11] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[1]))
            compute[13] = (compute[13] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[1]))
            produce pad_temp.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              pad_temp.shared[(threadIdx.x*7)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 447)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 1)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 446)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 2)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 445)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 3)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 444)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 4)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 443)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 5)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 442)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 6)] = tvm_if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (((blockIdx.x*112) + (threadIdx.x*7)) < 217)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 441)], 0f)
            }
            produce placeholder.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              if (likely((threadIdx.x < 2))) {
                if (likely((threadIdx.x < 2))) {
                  if (likely((threadIdx.x < 2))) {
                    if (likely((threadIdx.x < 2))) {
                      if (likely((threadIdx.x < 2))) {
                        if (likely((threadIdx.x < 2))) {
                          if (likely((((blockIdx.z*2) + threadIdx.x) < 10))) {
                            placeholder.shared[threadIdx.x] = placeholder[(((((blockIdx.z*150) + (threadIdx.x*75)) + (rc.outer*25)) + (ry.outer*5)) + 3)]
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
            compute[0] = (compute[0] + (pad_temp.shared[threadIdx.x]*placeholder.shared[0]))
            compute[2] = (compute[2] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[0]))
            compute[4] = (compute[4] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[0]))
            compute[6] = (compute[6] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[0]))
            compute[8] = (compute[8] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[0]))
            compute[10] = (compute[10] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[0]))
            compute[12] = (compute[12] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[0]))
            compute[1] = (compute[1] + (pad_temp.shared[threadIdx.x]*placeholder.shared[1]))
            compute[3] = (compute[3] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[1]))
            compute[5] = (compute[5] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[1]))
            compute[7] = (compute[7] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[1]))
            compute[9] = (compute[9] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[1]))
            compute[11] = (compute[11] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[1]))
            compute[13] = (compute[13] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[1]))
            produce pad_temp.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              pad_temp.shared[(threadIdx.x*7)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 446)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 1)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 445)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 2)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 444)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 3)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 443)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 4)] = tvm_if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 442)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 5)] = tvm_if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (((blockIdx.x*112) + (threadIdx.x*7)) < 217)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 441)], 0f)
              pad_temp.shared[((threadIdx.x*7) + 6)] = tvm_if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (((blockIdx.x*112) + (threadIdx.x*7)) < 216)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x*7)) - 440)], 0f)
            }
            produce placeholder.shared {
              // attr [iter_var(threadIdx.z, , threadIdx.z)] thread_extent = 1
              // attr [iter_var(threadIdx.y, , threadIdx.y)] thread_extent = 1
              // attr [iter_var(threadIdx.x, , threadIdx.x)] thread_extent = 16
              if (likely((threadIdx.x < 2))) {
                if (likely((threadIdx.x < 2))) {
                  if (likely((threadIdx.x < 2))) {
                    if (likely((threadIdx.x < 2))) {
                      if (likely((threadIdx.x < 2))) {
                        if (likely((threadIdx.x < 2))) {
                          if (likely((((blockIdx.z*2) + threadIdx.x) < 10))) {
                            placeholder.shared[threadIdx.x] = placeholder[(((((blockIdx.z*150) + (threadIdx.x*75)) + (rc.outer*25)) + (ry.outer*5)) + 4)]
                          }
                        }
                      }
                    }
                  }
                }
              }
            }
            compute[0] = (compute[0] + (pad_temp.shared[threadIdx.x]*placeholder.shared[0]))
            compute[2] = (compute[2] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[0]))
            compute[4] = (compute[4] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[0]))
            compute[6] = (compute[6] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[0]))
            compute[8] = (compute[8] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[0]))
            compute[10] = (compute[10] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[0]))
            compute[12] = (compute[12] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[0]))
            compute[1] = (compute[1] + (pad_temp.shared[threadIdx.x]*placeholder.shared[1]))
            compute[3] = (compute[3] + (pad_temp.shared[(threadIdx.x + 16)]*placeholder.shared[1]))
            compute[5] = (compute[5] + (pad_temp.shared[(threadIdx.x + 32)]*placeholder.shared[1]))
            compute[7] = (compute[7] + (pad_temp.shared[(threadIdx.x + 48)]*placeholder.shared[1]))
            compute[9] = (compute[9] + (pad_temp.shared[(threadIdx.x + 64)]*placeholder.shared[1]))
            compute[11] = (compute[11] + (pad_temp.shared[(threadIdx.x + 80)]*placeholder.shared[1]))
            compute[13] = (compute[13] + (pad_temp.shared[(threadIdx.x + 96)]*placeholder.shared[1]))
          }
        }
      }
      compute[((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x)] = max(compute[0], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 16)] = max(compute[2], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 32)] = max(compute[4], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 48)] = max(compute[6], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 64)] = max(compute[8], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 80)] = max(compute[10], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 96)] = max(compute[12], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50176)] = max(compute[1], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50192)] = max(compute[3], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50208)] = max(compute[5], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50224)] = max(compute[7], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50240)] = max(compute[9], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50256)] = max(compute[11], 0f)
      compute[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50272)] = max(compute[13], 0f)
    }




Summary
-------
In this tutorial, we have seen

- How to use TOPI API for common operations with numpy-style operators.
- How TOPI facilitates generic schedules and operator fusion for a context, to generate optimized kernel codes.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.492 seconds)


.. _sphx_glr_download_tutorials_topi_intro_topi.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: intro_topi.py <intro_topi.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: intro_topi.ipynb <intro_topi.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
