\hypertarget{namespacetopi_1_1nn}{}\section{topi\+:\+:nn Namespace Reference}
\label{namespacetopi_1_1nn}\index{topi\+::nn@{topi\+::nn}}
\subsection*{Enumerations}
\begin{DoxyCompactItemize}
\item 
enum \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} \+: int \{ \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95babbfb6c4315c8b57e558600af1515d3d8}{k\+Avg\+Pool}, 
\hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95ba3a406a1361a3c7ca311d3c514842c2f4}{k\+Max\+Pool}
 \}\begin{DoxyCompactList}\small\item\em Pooling type. \end{DoxyCompactList}
\end{DoxyCompactItemize}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \hyperlink{namespacetopi_1_1nn_a607b758a01a9350507e44670cecfbbb1}{batch\+\_\+matmul} (const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&x, const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&y)
\begin{DoxyCompactList}\small\item\em Creates an operation that calculates matrix multiplication in batch. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \hyperlink{namespacetopi_1_1nn_aff898fc8b6170d953782a01f6fc0e63f}{bias\+\_\+add} (const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&data, const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&bias, int axis)
\begin{DoxyCompactList}\small\item\em Creates an operation that calculates data + bias. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \hyperlink{namespacetopi_1_1nn_a01028bdee00ceef9f58fd29e4fceeaad}{binarize\+\_\+pack} (const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&data, int axis, std\+::string name=\char`\"{}Packed\+Input\char`\"{}, std\+::string tag=\char`\"{}binarize\+\_\+pack\char`\"{})
\begin{DoxyCompactList}\small\item\em Binarization and bit-\/packing along a certain axis. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \hyperlink{namespacetopi_1_1nn_acedfa473f37183df3c8188b667bb7594}{binary\+\_\+dense} (const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&data, const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&weight)
\begin{DoxyCompactList}\small\item\em Binary matrix multiplication using xor and bit-\/count. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \hyperlink{namespacetopi_1_1nn_abe55c065905a614f105da095a7f036ea}{dense} (const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&data, const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&weight, const \hyperlink{classtvm_1_1te_1_1Tensor}{tvm\+::te\+::\+Tensor} \&bias, const \hyperlink{namespacetvm_a41918af1a1dc386388639a9d3ad06c5d}{Data\+Type} \&out\+\_\+dtype)
\begin{DoxyCompactList}\small\item\em Creates an operation that calculates data $\ast$ weight$^\wedge$T + bias. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} \hyperlink{namespacetopi_1_1nn_ad57ccdad6f841aaafa2f64062f249a47}{all} (\hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ args)
\begin{DoxyCompactList}\small\item\em Create a new expression of the logical and of all conditions in the arguments. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a6ac795264be73287d92fcd7be8943ddc}{dilate} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ strides, std\+::string name=\char`\"{}tensor\char`\"{}, std\+::string tag=\hyperlink{namespacetopi_a60f05ec416e4618d25ad00dd9f536934}{k\+Injective})
\begin{DoxyCompactList}\small\item\em Dilate data with zeros. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a84054f5c8010db45e80365c3c2c6e1d4}{flatten} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, std\+::string name=\char`\"{}tensor\char`\"{}, std\+::string tag=\hyperlink{namespacetopi_a60f05ec416e4618d25ad00dd9f536934}{k\+Injective})
\begin{DoxyCompactList}\small\item\em Flattens the input tensor into a 2-\/D tensor by collapsing higher dimensions. This requires the input tensor to have constant sized dimensions. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a0ded232c2572637db6adc7cf5f0b35b2}{lrn} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&data, int size, int axis=1, float alpha=0.\+0001, float beta=0.\+75, float bias=2, std\+::string name=\char`\"{}tensor\char`\"{}, std\+::string tag=\hyperlink{namespacetopi_a794b9155e9ba9d1c9c42a1cff1fb645f}{k\+Broadcast})
\begin{DoxyCompactList}\small\item\em Local response normalization inference operator. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_ae9516b844130a2189ad2989b646ec2b1}{scale\+\_\+shift\+\_\+nchw} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&scale, const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&shift, std\+::string name=\char`\"{}Scale\+Shift\char`\"{}, std\+::string tag=\hyperlink{namespacetopi_a794b9155e9ba9d1c9c42a1cff1fb645f}{k\+Broadcast})
\begin{DoxyCompactList}\small\item\em Scale and shift with N\+C\+HW order. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_ab19179f1b4a73a42b17ddf2f5fea46fc}{scale\+\_\+shift\+\_\+nhwc} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&scale, const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&shift, std\+::string name=\char`\"{}Scale\+Shift\char`\"{}, std\+::string tag=\hyperlink{namespacetopi_a794b9155e9ba9d1c9c42a1cff1fb645f}{k\+Broadcast})
\begin{DoxyCompactList}\small\item\em Scale and shift with N\+H\+WC order. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_ad51533b09956d7bc8de2537adf3b6b77}{pool\+\_\+impl} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&kernel\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&stride\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&padding\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, bool ceil\+\_\+mode, const size\+\_\+t height\+\_\+axis, const size\+\_\+t width\+\_\+axis, bool count\+\_\+include\+\_\+pad)
\begin{DoxyCompactList}\small\item\em Perform pooling on height and width dimension of data. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_af2c25e8b3ab3cac1c2896cb750838337}{pool\+\_\+grad\+\_\+impl} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&out\+\_\+grad, const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&kernel\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&stride\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&padding\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, bool ceil\+\_\+mode, const size\+\_\+t height\+\_\+axis, const size\+\_\+t width\+\_\+axis, bool count\+\_\+include\+\_\+pad)
\item 
bool \hyperlink{namespacetopi_1_1nn_a2e81a7938a1e3f273e184e2373d9138d}{find\+\_\+depth\+\_\+height\+\_\+width} (const std\+::string \&layout, int $\ast$depth\+\_\+axis, int $\ast$height\+\_\+axis, int $\ast$width\+\_\+axis)
\item 
bool \hyperlink{namespacetopi_1_1nn_a428e0ba6800ef89b8c1f97f0245e244d}{find\+\_\+height\+\_\+width} (const std\+::string \&layout, int $\ast$height\+\_\+axis, int $\ast$width\+\_\+axis)
\item 
bool \hyperlink{namespacetopi_1_1nn_ab1f1f9f86723b30bb8997615e1d63ca8}{find\+\_\+width} (const std\+::string \&layout, int $\ast$width\+\_\+axis)
\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_ac1708b3aa1a677f56a4063a568945d98}{pool} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&kernel\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&stride\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&padding\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=\char`\"{}N\+C\+HW\char`\"{}, bool count\+\_\+include\+\_\+pad=true)
\begin{DoxyCompactList}\small\item\em Perform pooling on height and width dimension of data. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, etc. are valid for pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a4f915567f195ade4a17743a5e7654e88}{pool\+\_\+grad} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&out\+\_\+grad, const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&kernel\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&stride\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&padding\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=\char`\"{}N\+C\+HW\char`\"{}, bool count\+\_\+include\+\_\+pad=true)
\begin{DoxyCompactList}\small\item\em Calculate gradient of pooling on height and width dimension of data. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, etc. are valid for pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} \hyperlink{namespacetopi_1_1nn_a91b52c68356d23123474ebf10f9b0140}{start\+\_\+index} (const \hyperlink{classtvm_1_1tir_1_1Var}{Var} \&out\+\_\+index, const \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} \&odim, const \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} \&idim)
\item 
\hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} \hyperlink{namespacetopi_1_1nn_aadbaaec56f0b485262bf5199bbe3dcb3}{end\+\_\+index} (const \hyperlink{classtvm_1_1tir_1_1Var}{Var} \&out\+\_\+index, const \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} \&odim, const \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} \&idim)
\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_ad4f34df5cfa8dc75843116bc39f06066}{adaptive\+\_\+pool\+\_\+impl} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&output\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, const std\+::vector$<$ int $>$ \&axes)
\begin{DoxyCompactList}\small\item\em Perform adaptive pooling on N dimensional data. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_af721a019c13f1f99dc43d5d49cc71388}{adaptive\+\_\+pool} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&output\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, const std\+::string \&layout=\char`\"{}N\+C\+HW\char`\"{})
\begin{DoxyCompactList}\small\item\em Adaptively perform pooling on height and width dimension of data. The pooling kernel and stride sizes are automatically chosen for desired output sizes. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, etc. are valid for pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_ad48c57c26ce6bb02576555a4cb11bcd3}{adaptive\+\_\+pool3d} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&output\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, const std\+::string \&layout=\char`\"{}N\+C\+D\+HW\char`\"{})
\begin{DoxyCompactList}\small\item\em Adaptively perform pooling on three dimensional data. See the two dimensional version above for details. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_ac5fe64687aa8bffee420bf282f2b8f8c}{global\+\_\+pool} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, const std\+::string \&layout=\char`\"{}N\+C\+HW\char`\"{})
\begin{DoxyCompactList}\small\item\em Perform global pooling on height and width dimension of data. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, ... are valid for global\+\_\+pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a0b2681e29b1f733835ffe2e6b3b69c13}{pool\+\_\+impl\+\_\+nd} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&kernel\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&stride\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&padding\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, bool ceil\+\_\+mode, const std\+::vector$<$ int $>$ \&axis, bool count\+\_\+include\+\_\+pad)
\begin{DoxyCompactList}\small\item\em Perform pooling on N-\/dimension of data. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a379dfcc1d33774fb4ce998550dda187c}{pool1d} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&kernel\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&stride\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&padding\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=\char`\"{}N\+CW\char`\"{}, bool count\+\_\+include\+\_\+pad=true)
\begin{DoxyCompactList}\small\item\em Perform pooling on the width dimension of data. Width axis is determined by the layout string in which \textquotesingle{}W\textquotesingle{} means width. Width dimension cannot be split. For example, N\+CW, N\+C\+W16c, etc. are valid for pool, while N\+C\+W16w is not. See {\itshape layout} for more information of the layout string convention. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_af84c2ac0c1fd4ec7db5c9bc661bd5aab}{pool3d} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&kernel\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&stride\+\_\+size, const \hyperlink{classtvm_1_1Array}{Array}$<$ \hyperlink{classtvm_1_1PrimExpr}{Prim\+Expr} $>$ \&padding\+\_\+size, \hyperlink{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{Pool\+Type} pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=\char`\"{}N\+C\+D\+HW\char`\"{}, bool count\+\_\+include\+\_\+pad=true)
\begin{DoxyCompactList}\small\item\em Perform pooling on depth, height and width dimension of data. It decides the depth, height and width dimension according to the layout string, in which \textquotesingle{}D\textquotesingle{}, \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means depth, width and height respectively. Depth, Width and height dimension cannot be split. For example, N\+C\+D\+HW, N\+C\+D\+H\+W16c, etc. are valid for pool, while N\+C\+D\+H\+W16d, N\+C\+D\+H\+W16w or N\+C\+D\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a1c845f2e02a677c556929d41a399e729}{softmax} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, int axis=-\/1, std\+::string name=\char`\"{}tensor\char`\"{}, std\+::string tag=\char`\"{}softmax\+\_\+output\char`\"{})
\begin{DoxyCompactList}\small\item\em Softmax activation. \end{DoxyCompactList}\item 
\hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \hyperlink{namespacetopi_1_1nn_a667b9b98da3fd9a918a603d1b8aad5d7}{log\+\_\+softmax} (const \hyperlink{classtvm_1_1te_1_1Tensor}{Tensor} \&x, std\+::string name=\char`\"{}tensor\char`\"{}, std\+::string tag=\char`\"{}log\+\_\+softmax\+\_\+output\char`\"{})
\begin{DoxyCompactList}\small\item\em Log softmax activation. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Enumeration Type Documentation}
\index{topi\+::nn@{topi\+::nn}!Pool\+Type@{Pool\+Type}}
\index{Pool\+Type@{Pool\+Type}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{Pool\+Type}{PoolType}}]{\setlength{\rightskip}{0pt plus 5cm}enum {\bf topi\+::nn\+::\+Pool\+Type} \+: int}\hypertarget{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}{}\label{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95b}


Pooling type. 

\begin{Desc}
\item[Enumerator]\par
\begin{description}
\index{k\+Avg\+Pool@{k\+Avg\+Pool}!topi\+::nn@{topi\+::nn}}\index{topi\+::nn@{topi\+::nn}!k\+Avg\+Pool@{k\+Avg\+Pool}}\item[{\em 
k\+Avg\+Pool\hypertarget{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95babbfb6c4315c8b57e558600af1515d3d8}{}\label{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95babbfb6c4315c8b57e558600af1515d3d8}
}]\index{k\+Max\+Pool@{k\+Max\+Pool}!topi\+::nn@{topi\+::nn}}\index{topi\+::nn@{topi\+::nn}!k\+Max\+Pool@{k\+Max\+Pool}}\item[{\em 
k\+Max\+Pool\hypertarget{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95ba3a406a1361a3c7ca311d3c514842c2f4}{}\label{namespacetopi_1_1nn_ac531cfce9c3a031fa25cfb6ed1f9b95ba3a406a1361a3c7ca311d3c514842c2f4}
}]\end{description}
\end{Desc}


\subsection{Function Documentation}
\index{topi\+::nn@{topi\+::nn}!adaptive\+\_\+pool@{adaptive\+\_\+pool}}
\index{adaptive\+\_\+pool@{adaptive\+\_\+pool}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{adaptive\+\_\+pool(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&output\+\_\+size, Pool\+Type pool\+\_\+type, const std\+::string \&layout=""N\+C\+HW"")}{adaptive_pool(const Tensor &x, const Array< PrimExpr > &output_size, PoolType pool_type, const std::string &layout="NCHW")}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::adaptive\+\_\+pool (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{output\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{const std\+::string \&}]{layout = {\ttfamily \char`\"{}NCHW\char`\"{}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_af721a019c13f1f99dc43d5d49cc71388}{}\label{namespacetopi_1_1nn_af721a019c13f1f99dc43d5d49cc71388}


Adaptively perform pooling on height and width dimension of data. The pooling kernel and stride sizes are automatically chosen for desired output sizes. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, etc. are valid for pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor \\
\hline
{\em output\+\_\+size} & Vector of two ints\+: \{output\+\_\+height, output\+\_\+width\} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em layout} & The input layout. Pooling supports any layout as long as \textquotesingle{}H\textquotesingle{} and \textquotesingle{}W\textquotesingle{} appear. The layout is supposed to be composed of upper cases, lower cases and (optional) numbers, where upper case indicates a dimension and the corresponding lower case (with factor size) indicates the split dimension. For example, N\+C\+H\+W16c can describe a 5-\/D tensor of \mbox{[}batch\+\_\+size, channel, height, width, channel\+\_\+block\mbox{]}. (in which factor size {\ttfamily 16} will not be used in pooling but for other operators, it can be used to decide the output shape). Since pooling does not care about the factor size of dimensions other than {\ttfamily H} and {\ttfamily W}, one can pass {\ttfamily N\+C\+H\+Wc} as well.\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in same layout order 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!adaptive\+\_\+pool3d@{adaptive\+\_\+pool3d}}
\index{adaptive\+\_\+pool3d@{adaptive\+\_\+pool3d}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{adaptive\+\_\+pool3d(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&output\+\_\+size, Pool\+Type pool\+\_\+type, const std\+::string \&layout=""N\+C\+D\+HW"")}{adaptive_pool3d(const Tensor &x, const Array< PrimExpr > &output_size, PoolType pool_type, const std::string &layout="NCDHW")}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::adaptive\+\_\+pool3d (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{output\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{const std\+::string \&}]{layout = {\ttfamily \char`\"{}NCDHW\char`\"{}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ad48c57c26ce6bb02576555a4cb11bcd3}{}\label{namespacetopi_1_1nn_ad48c57c26ce6bb02576555a4cb11bcd3}


Adaptively perform pooling on three dimensional data. See the two dimensional version above for details. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor \\
\hline
{\em output\+\_\+size} & Vector of three ints\+: \{output\+\_\+depth, output\+\_\+height, output\+\_\+width\} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em layout} & The input layout. The default is \char`\"{}\+N\+C\+D\+H\+W\char`\"{}. \\
\hline
\end{DoxyParams}
\index{topi\+::nn@{topi\+::nn}!adaptive\+\_\+pool\+\_\+impl@{adaptive\+\_\+pool\+\_\+impl}}
\index{adaptive\+\_\+pool\+\_\+impl@{adaptive\+\_\+pool\+\_\+impl}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{adaptive\+\_\+pool\+\_\+impl(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&output\+\_\+size, Pool\+Type pool\+\_\+type, const std\+::vector$<$ int $>$ \&axes)}{adaptive_pool_impl(const Tensor &x, const Array< PrimExpr > &output_size, PoolType pool_type, const std::vector< int > &axes)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::adaptive\+\_\+pool\+\_\+impl (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{output\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{const std\+::vector$<$ int $>$ \&}]{axes}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ad4f34df5cfa8dc75843116bc39f06066}{}\label{namespacetopi_1_1nn_ad4f34df5cfa8dc75843116bc39f06066}


Perform adaptive pooling on N dimensional data. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor \\
\hline
{\em output\+\_\+size} & int vector of size in each dimension \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em axes} & indices of each dimension\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in same layout order 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!all@{all}}
\index{all@{all}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{all(\+Array$<$ Prim\+Expr $>$ args)}{all(Array< PrimExpr > args)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Prim\+Expr} topi\+::nn\+::all (
\begin{DoxyParamCaption}
\item[{{\bf Array}$<$ {\bf Prim\+Expr} $>$}]{args}
\end{DoxyParamCaption}
)}\hypertarget{namespacetopi_1_1nn_ad57ccdad6f841aaafa2f64062f249a47}{}\label{namespacetopi_1_1nn_ad57ccdad6f841aaafa2f64062f249a47}


Create a new expression of the logical and of all conditions in the arguments. 


\begin{DoxyParams}{Parameters}
{\em args} & The arguments to find the logical conjunction of\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The logical conjunction expression 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!batch\+\_\+matmul@{batch\+\_\+matmul}}
\index{batch\+\_\+matmul@{batch\+\_\+matmul}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{batch\+\_\+matmul(const tvm\+::te\+::\+Tensor \&x, const tvm\+::te\+::\+Tensor \&y)}{batch_matmul(const tvm::te::Tensor &x, const tvm::te::Tensor &y)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf tvm\+::te\+::\+Tensor} topi\+::nn\+::batch\+\_\+matmul (
\begin{DoxyParamCaption}
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{x, }
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{y}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a607b758a01a9350507e44670cecfbbb1}{}\label{namespacetopi_1_1nn_a607b758a01a9350507e44670cecfbbb1}


Creates an operation that calculates matrix multiplication in batch. 


\begin{DoxyParams}{Parameters}
{\em x} & Tensor with shape \mbox{[}batch, M, K\mbox{]} \\
\hline
{\em y} & Tensor with shape \mbox{[}batch, N, K\mbox{]}\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Tensor with shape \mbox{[}batch, M, N\mbox{]} 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!bias\+\_\+add@{bias\+\_\+add}}
\index{bias\+\_\+add@{bias\+\_\+add}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{bias\+\_\+add(const tvm\+::te\+::\+Tensor \&data, const tvm\+::te\+::\+Tensor \&bias, int axis)}{bias_add(const tvm::te::Tensor &data, const tvm::te::Tensor &bias, int axis)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf tvm\+::te\+::\+Tensor} topi\+::nn\+::bias\+\_\+add (
\begin{DoxyParamCaption}
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{data, }
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{bias, }
\item[{int}]{axis}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_aff898fc8b6170d953782a01f6fc0e63f}{}\label{namespacetopi_1_1nn_aff898fc8b6170d953782a01f6fc0e63f}


Creates an operation that calculates data + bias. 


\begin{DoxyParams}{Parameters}
{\em data} & Tensor with shape \mbox{[}batch, in\+\_\+dim\mbox{]} \\
\hline
{\em bias} & Tensor with shape \mbox{[}batch\mbox{]}. \\
\hline
{\em axis} & The axis to add the bias to. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Tensor with shape \mbox{[}batch, in\+\_\+dim\mbox{]} 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!binarize\+\_\+pack@{binarize\+\_\+pack}}
\index{binarize\+\_\+pack@{binarize\+\_\+pack}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{binarize\+\_\+pack(const tvm\+::te\+::\+Tensor \&data, int axis, std\+::string name=""Packed\+Input"", std\+::string tag=""binarize\+\_\+pack"")}{binarize_pack(const tvm::te::Tensor &data, int axis, std::string name="PackedInput", std::string tag="binarize_pack")}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf tvm\+::te\+::\+Tensor} topi\+::nn\+::binarize\+\_\+pack (
\begin{DoxyParamCaption}
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{data, }
\item[{int}]{axis, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}PackedInput\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily \char`\"{}binarize\+\_\+pack\char`\"{}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a01028bdee00ceef9f58fd29e4fceeaad}{}\label{namespacetopi_1_1nn_a01028bdee00ceef9f58fd29e4fceeaad}


Binarization and bit-\/packing along a certain axis. 


\begin{DoxyParams}{Parameters}
{\em data} & N-\/D tensor, can be any layout \\
\hline
{\em axis} & The axis along which to do binarization and bit-\/packing. This axis must have a size equal to an integer multiple of 32. \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Output tensor with dtype uint32 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!binary\+\_\+dense@{binary\+\_\+dense}}
\index{binary\+\_\+dense@{binary\+\_\+dense}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{binary\+\_\+dense(const tvm\+::te\+::\+Tensor \&data, const tvm\+::te\+::\+Tensor \&weight)}{binary_dense(const tvm::te::Tensor &data, const tvm::te::Tensor &weight)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf tvm\+::te\+::\+Tensor} topi\+::nn\+::binary\+\_\+dense (
\begin{DoxyParamCaption}
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{data, }
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{weight}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_acedfa473f37183df3c8188b667bb7594}{}\label{namespacetopi_1_1nn_acedfa473f37183df3c8188b667bb7594}


Binary matrix multiplication using xor and bit-\/count. 


\begin{DoxyParams}{Parameters}
{\em data} & Tensor with shape \mbox{[}batch, in\+\_\+dim\mbox{]}, dtype is uint32 \\
\hline
{\em weight} & Tensor with shape \mbox{[}out\+\_\+dim, in\+\_\+dim\mbox{]}, dtype is uint32\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Tensor with shape \mbox{[}batch, out\+\_\+dim\mbox{]}, dtype is float32 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!dense@{dense}}
\index{dense@{dense}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{dense(const tvm\+::te\+::\+Tensor \&data, const tvm\+::te\+::\+Tensor \&weight, const tvm\+::te\+::\+Tensor \&bias, const Data\+Type \&out\+\_\+dtype)}{dense(const tvm::te::Tensor &data, const tvm::te::Tensor &weight, const tvm::te::Tensor &bias, const DataType &out_dtype)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf tvm\+::te\+::\+Tensor} topi\+::nn\+::dense (
\begin{DoxyParamCaption}
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{data, }
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{weight, }
\item[{const {\bf tvm\+::te\+::\+Tensor} \&}]{bias, }
\item[{const {\bf Data\+Type} \&}]{out\+\_\+dtype}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_abe55c065905a614f105da095a7f036ea}{}\label{namespacetopi_1_1nn_abe55c065905a614f105da095a7f036ea}


Creates an operation that calculates data $\ast$ weight$^\wedge$T + bias. 


\begin{DoxyParams}{Parameters}
{\em data} & Tensor with shape \mbox{[}batch, in\+\_\+dim\mbox{]} \\
\hline
{\em weight} & Tensor with shape \mbox{[}out\+\_\+dim, in\+\_\+dim\mbox{]} \\
\hline
{\em bias} & Tensor with shape \mbox{[}out\+\_\+dim\mbox{]}. Optional; to omit bias, pass Tensor() \\
\hline
{\em out\+\_\+dtype} & Output data type. Used for mixed precision.\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Tensor with shape \mbox{[}batch, out\+\_\+dim\mbox{]} 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!dilate@{dilate}}
\index{dilate@{dilate}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{dilate(const Tensor \&x, Array$<$ Prim\+Expr $>$ strides, std\+::string name=""tensor"", std\+::string tag=k\+Injective)}{dilate(const Tensor &x, Array< PrimExpr > strides, std::string name="tensor", std::string tag=kInjective)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::dilate (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{{\bf Array}$<$ {\bf Prim\+Expr} $>$}]{strides, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}tensor\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily {\bf k\+Injective}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a6ac795264be73287d92fcd7be8943ddc}{}\label{namespacetopi_1_1nn_a6ac795264be73287d92fcd7be8943ddc}


Dilate data with zeros. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor, this can have any number of dimensions and any layout. \\
\hline
{\em strides} & Dilation stride for each dimension. Stride 1 means no dilation. \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor. 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!end\+\_\+index@{end\+\_\+index}}
\index{end\+\_\+index@{end\+\_\+index}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{end\+\_\+index(const Var \&out\+\_\+index, const Prim\+Expr \&odim, const Prim\+Expr \&idim)}{end_index(const Var &out_index, const PrimExpr &odim, const PrimExpr &idim)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Prim\+Expr} topi\+::nn\+::end\+\_\+index (
\begin{DoxyParamCaption}
\item[{const {\bf Var} \&}]{out\+\_\+index, }
\item[{const {\bf Prim\+Expr} \&}]{odim, }
\item[{const {\bf Prim\+Expr} \&}]{idim}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_aadbaaec56f0b485262bf5199bbe3dcb3}{}\label{namespacetopi_1_1nn_aadbaaec56f0b485262bf5199bbe3dcb3}
\index{topi\+::nn@{topi\+::nn}!find\+\_\+depth\+\_\+height\+\_\+width@{find\+\_\+depth\+\_\+height\+\_\+width}}
\index{find\+\_\+depth\+\_\+height\+\_\+width@{find\+\_\+depth\+\_\+height\+\_\+width}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{find\+\_\+depth\+\_\+height\+\_\+width(const std\+::string \&layout, int $\ast$depth\+\_\+axis, int $\ast$height\+\_\+axis, int $\ast$width\+\_\+axis)}{find_depth_height_width(const std::string &layout, int *depth_axis, int *height_axis, int *width_axis)}}]{\setlength{\rightskip}{0pt plus 5cm}bool topi\+::nn\+::find\+\_\+depth\+\_\+height\+\_\+width (
\begin{DoxyParamCaption}
\item[{const std\+::string \&}]{layout, }
\item[{int $\ast$}]{depth\+\_\+axis, }
\item[{int $\ast$}]{height\+\_\+axis, }
\item[{int $\ast$}]{width\+\_\+axis}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a2e81a7938a1e3f273e184e2373d9138d}{}\label{namespacetopi_1_1nn_a2e81a7938a1e3f273e184e2373d9138d}
\index{topi\+::nn@{topi\+::nn}!find\+\_\+height\+\_\+width@{find\+\_\+height\+\_\+width}}
\index{find\+\_\+height\+\_\+width@{find\+\_\+height\+\_\+width}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{find\+\_\+height\+\_\+width(const std\+::string \&layout, int $\ast$height\+\_\+axis, int $\ast$width\+\_\+axis)}{find_height_width(const std::string &layout, int *height_axis, int *width_axis)}}]{\setlength{\rightskip}{0pt plus 5cm}bool topi\+::nn\+::find\+\_\+height\+\_\+width (
\begin{DoxyParamCaption}
\item[{const std\+::string \&}]{layout, }
\item[{int $\ast$}]{height\+\_\+axis, }
\item[{int $\ast$}]{width\+\_\+axis}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a428e0ba6800ef89b8c1f97f0245e244d}{}\label{namespacetopi_1_1nn_a428e0ba6800ef89b8c1f97f0245e244d}
\index{topi\+::nn@{topi\+::nn}!find\+\_\+width@{find\+\_\+width}}
\index{find\+\_\+width@{find\+\_\+width}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{find\+\_\+width(const std\+::string \&layout, int $\ast$width\+\_\+axis)}{find_width(const std::string &layout, int *width_axis)}}]{\setlength{\rightskip}{0pt plus 5cm}bool topi\+::nn\+::find\+\_\+width (
\begin{DoxyParamCaption}
\item[{const std\+::string \&}]{layout, }
\item[{int $\ast$}]{width\+\_\+axis}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ab1f1f9f86723b30bb8997615e1d63ca8}{}\label{namespacetopi_1_1nn_ab1f1f9f86723b30bb8997615e1d63ca8}
\index{topi\+::nn@{topi\+::nn}!flatten@{flatten}}
\index{flatten@{flatten}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{flatten(const Tensor \&x, std\+::string name=""tensor"", std\+::string tag=k\+Injective)}{flatten(const Tensor &x, std::string name="tensor", std::string tag=kInjective)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::flatten (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}tensor\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily {\bf k\+Injective}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a84054f5c8010db45e80365c3c2c6e1d4}{}\label{namespacetopi_1_1nn_a84054f5c8010db45e80365c3c2c6e1d4}


Flattens the input tensor into a 2-\/D tensor by collapsing higher dimensions. This requires the input tensor to have constant sized dimensions. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A 2-\/D tensor. 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!global\+\_\+pool@{global\+\_\+pool}}
\index{global\+\_\+pool@{global\+\_\+pool}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{global\+\_\+pool(const Tensor \&x, Pool\+Type pool\+\_\+type, const std\+::string \&layout=""N\+C\+HW"")}{global_pool(const Tensor &x, PoolType pool_type, const std::string &layout="NCHW")}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::global\+\_\+pool (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{const std\+::string \&}]{layout = {\ttfamily \char`\"{}NCHW\char`\"{}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ac5fe64687aa8bffee420bf282f2b8f8c}{}\label{namespacetopi_1_1nn_ac5fe64687aa8bffee420bf282f2b8f8c}


Perform global pooling on height and width dimension of data. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, ... are valid for global\+\_\+pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor represent as layout \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em layout} & The input layout. global-\/pooling supports any layout as long as \textquotesingle{}H\textquotesingle{} and \textquotesingle{}W\textquotesingle{} appear. The layout is supposed to be composed of upper cases, lower cases and (optional) numbers, where upper case indicates a dimension and the corresponding lower case (with factor size) indicates the sub-\/dimension. For example, {\ttfamily N\+C\+H\+W16c} can describe a 5-\/D tensor of \mbox{[}batch\+\_\+size, channel, height, width, channel\+\_\+block\mbox{]}. (in which factor size {\ttfamily 16} will not be used in pooling but for other operators, it can be used to decide the output shape). Since pooling does not care about the factor size of dimensions other than {\ttfamily H} and {\ttfamily W}, one can pass {\ttfamily N\+C\+H\+Wc} as well.\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in same layout with height and width dimension size of 1. e.\+g., for N\+C\+HW, the output shape will be \mbox{[}batch, channel, 1, 1\mbox{]} 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!log\+\_\+softmax@{log\+\_\+softmax}}
\index{log\+\_\+softmax@{log\+\_\+softmax}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{log\+\_\+softmax(const Tensor \&x, std\+::string name=""tensor"", std\+::string tag=""log\+\_\+softmax\+\_\+output"")}{log_softmax(const Tensor &x, std::string name="tensor", std::string tag="log_softmax_output")}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::log\+\_\+softmax (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}tensor\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily \char`\"{}log\+\_\+softmax\+\_\+output\char`\"{}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a667b9b98da3fd9a918a603d1b8aad5d7}{}\label{namespacetopi_1_1nn_a667b9b98da3fd9a918a603d1b8aad5d7}


Log softmax activation. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. 2-\/D where log softmax is performed along the second dimension \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A Tensor whose op member is the log softmax operation 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!lrn@{lrn}}
\index{lrn@{lrn}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{lrn(const Tensor \&data, int size, int axis=1, float alpha=0.\+0001, float beta=0.\+75, float bias=2, std\+::string name=""tensor"", std\+::string tag=k\+Broadcast)}{lrn(const Tensor &data, int size, int axis=1, float alpha=0.0001, float beta=0.75, float bias=2, std::string name="tensor", std::string tag=kBroadcast)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::lrn (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{data, }
\item[{int}]{size, }
\item[{int}]{axis = {\ttfamily 1}, }
\item[{float}]{alpha = {\ttfamily 0.0001}, }
\item[{float}]{beta = {\ttfamily 0.75}, }
\item[{float}]{bias = {\ttfamily 2}, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}tensor\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily {\bf k\+Broadcast}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a0ded232c2572637db6adc7cf5f0b35b2}{}\label{namespacetopi_1_1nn_a0ded232c2572637db6adc7cf5f0b35b2}


Local response normalization inference operator. 


\begin{DoxyParams}{Parameters}
{\em data} & The input tensor. 4-\/D shape N\+C\+HW or N\+H\+WC \\
\hline
{\em size} & Integer to define normalisation window size \\
\hline
{\em axis} & Input data layout channel axis \\
\hline
{\em alpha} & Float scaling factor \\
\hline
{\em beta} & Exponent value \\
\hline
{\em bias} & Offset to avoid dividing by zero \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A Tensor whose op member is the Local response normalization operation 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!pool@{pool}}
\index{pool@{pool}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{pool(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&kernel\+\_\+size, const Array$<$ Prim\+Expr $>$ \&stride\+\_\+size, const Array$<$ Prim\+Expr $>$ \&padding\+\_\+size, Pool\+Type pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=""N\+C\+HW"", bool count\+\_\+include\+\_\+pad=true)}{pool(const Tensor &x, const Array< PrimExpr > &kernel_size, const Array< PrimExpr > &stride_size, const Array< PrimExpr > &padding_size, PoolType pool_type, bool ceil_mode, const std::string &layout="NCHW", bool count_include_pad=true)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::pool (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{kernel\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{stride\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{padding\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{bool}]{ceil\+\_\+mode, }
\item[{const std\+::string \&}]{layout = {\ttfamily \char`\"{}NCHW\char`\"{}}, }
\item[{bool}]{count\+\_\+include\+\_\+pad = {\ttfamily true}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ac1708b3aa1a677f56a4063a568945d98}{}\label{namespacetopi_1_1nn_ac1708b3aa1a677f56a4063a568945d98}


Perform pooling on height and width dimension of data. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, etc. are valid for pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. \\
\hline
{\em kernel\+\_\+size} & Vector of two ints\+: \{kernel\+\_\+height, kernel\+\_\+width\} \\
\hline
{\em stride\+\_\+size} & Vector of two ints\+: \{stride\+\_\+height, stride\+\_\+width\} \\
\hline
{\em padding\+\_\+size} & Vector of two ints\+: \{padding\+\_\+height, padding\+\_\+width\} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em ceil\+\_\+mode} & Whether to use ceil when calculating the output size \\
\hline
{\em layout} & The input layout. Pooling supports any layout as long as \textquotesingle{}H\textquotesingle{} and \textquotesingle{}W\textquotesingle{} appear. The layout is supposed to be composed of upper cases, lower cases and (optional) numbers, where upper case indicates a dimension and the corresponding lower case (with factor size) indicates the split dimension. For example, N\+C\+H\+W16c can describe a 5-\/D tensor of \mbox{[}batch\+\_\+size, channel, height, width, channel\+\_\+block\mbox{]}. (in which factor size {\ttfamily 16} will not be used in pooling but for other operators, it can be used to decide the output shape). Since pooling does not care about the factor size of dimensions other than {\ttfamily H} and {\ttfamily W}, one can pass {\ttfamily N\+C\+H\+Wc} as well. \\
\hline
{\em count\+\_\+include\+\_\+pad} & Whether include padding in the calculation when pool\+\_\+type is \textquotesingle{}avg\textquotesingle{}\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in the same layout 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!pool1d@{pool1d}}
\index{pool1d@{pool1d}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{pool1d(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&kernel\+\_\+size, const Array$<$ Prim\+Expr $>$ \&stride\+\_\+size, const Array$<$ Prim\+Expr $>$ \&padding\+\_\+size, Pool\+Type pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=""N\+CW"", bool count\+\_\+include\+\_\+pad=true)}{pool1d(const Tensor &x, const Array< PrimExpr > &kernel_size, const Array< PrimExpr > &stride_size, const Array< PrimExpr > &padding_size, PoolType pool_type, bool ceil_mode, const std::string &layout="NCW", bool count_include_pad=true)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::pool1d (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{kernel\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{stride\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{padding\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{bool}]{ceil\+\_\+mode, }
\item[{const std\+::string \&}]{layout = {\ttfamily \char`\"{}NCW\char`\"{}}, }
\item[{bool}]{count\+\_\+include\+\_\+pad = {\ttfamily true}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a379dfcc1d33774fb4ce998550dda187c}{}\label{namespacetopi_1_1nn_a379dfcc1d33774fb4ce998550dda187c}


Perform pooling on the width dimension of data. Width axis is determined by the layout string in which \textquotesingle{}W\textquotesingle{} means width. Width dimension cannot be split. For example, N\+CW, N\+C\+W16c, etc. are valid for pool, while N\+C\+W16w is not. See {\itshape layout} for more information of the layout string convention. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. \\
\hline
{\em kernel\+\_\+size} & Vector of three ints\+: \{kernel\+\_\+width\} \\
\hline
{\em stride\+\_\+size} & Vector of three ints\+: \{stride\+\_\+width\} \\
\hline
{\em padding\+\_\+size} & Vector of six ints\+: \{head\+\_\+pad\+\_\+width, tail\+\_\+pad\+\_\+width\} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em ceil\+\_\+mode} & Whether to use ceil when calculating the output size \\
\hline
{\em layout} & The input layout. Pooling supports any layout as long as \textquotesingle{}W\textquotesingle{} appears. The layout is supposed to be composed of upper cases, lower cases and (optional) numbers, where upper case indicates a dimension and the corresponding lower case (with factor size) indicates the split dimension. For example, N\+C\+W16c can describe a 4-\/D tensor of \mbox{[}batch\+\_\+size, channel, width, channel\+\_\+block\mbox{]}. (in which factor size {\ttfamily 16} will not be used in pooling but for other operators, it can be used to decide the output shape). Since pooling does not care about the factor size of dimensions other than {\ttfamily W}, one can pass {\ttfamily N\+C\+Wc} as well. \\
\hline
{\em count\+\_\+include\+\_\+pad} & Whether include padding in the calculation when pool\+\_\+type is \textquotesingle{}avg\textquotesingle{}\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in the same layout 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!pool3d@{pool3d}}
\index{pool3d@{pool3d}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{pool3d(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&kernel\+\_\+size, const Array$<$ Prim\+Expr $>$ \&stride\+\_\+size, const Array$<$ Prim\+Expr $>$ \&padding\+\_\+size, Pool\+Type pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=""N\+C\+D\+HW"", bool count\+\_\+include\+\_\+pad=true)}{pool3d(const Tensor &x, const Array< PrimExpr > &kernel_size, const Array< PrimExpr > &stride_size, const Array< PrimExpr > &padding_size, PoolType pool_type, bool ceil_mode, const std::string &layout="NCDHW", bool count_include_pad=true)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::pool3d (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{kernel\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{stride\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{padding\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{bool}]{ceil\+\_\+mode, }
\item[{const std\+::string \&}]{layout = {\ttfamily \char`\"{}NCDHW\char`\"{}}, }
\item[{bool}]{count\+\_\+include\+\_\+pad = {\ttfamily true}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_af84c2ac0c1fd4ec7db5c9bc661bd5aab}{}\label{namespacetopi_1_1nn_af84c2ac0c1fd4ec7db5c9bc661bd5aab}


Perform pooling on depth, height and width dimension of data. It decides the depth, height and width dimension according to the layout string, in which \textquotesingle{}D\textquotesingle{}, \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means depth, width and height respectively. Depth, Width and height dimension cannot be split. For example, N\+C\+D\+HW, N\+C\+D\+H\+W16c, etc. are valid for pool, while N\+C\+D\+H\+W16d, N\+C\+D\+H\+W16w or N\+C\+D\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. \\
\hline
{\em kernel\+\_\+size} & Vector of three ints\+: \{kernel\+\_\+depth, kernel\+\_\+height, kernel\+\_\+width\} \\
\hline
{\em stride\+\_\+size} & Vector of three ints\+: \{stride\+\_\+depth, stride\+\_\+height, stride\+\_\+width\} \\
\hline
{\em padding\+\_\+size} & Vector of six ints\+: \{head\+\_\+pad\+\_\+depth, head\+\_\+pad\+\_\+height, head\+\_\+pad\+\_\+width, tail\+\_\+pad\+\_\+depth, tail\+\_\+pad\+\_\+height, tail\+\_\+pad\+\_\+width\} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em ceil\+\_\+mode} & Whether to use ceil when calculating the output size \\
\hline
{\em layout} & The input layout. Pooling supports any layout as long as \textquotesingle{}D\textquotesingle{}, \textquotesingle{}H\textquotesingle{} and \textquotesingle{}W\textquotesingle{} appear. The layout is supposed to be composed of upper cases, lower cases and (optional) numbers, where upper case indicates a dimension and the corresponding lower case (with factor size) indicates the split dimension. For example, N\+C\+D\+H\+W16c can describe a 6-\/D tensor of \mbox{[}batch\+\_\+size, channel, depth, height, width, channel\+\_\+block\mbox{]}. (in which factor size {\ttfamily 16} will not be used in pooling but for other operators, it can be used to decide the output shape). Since pooling does not care about the factor size of dimensions other than {\ttfamily D}, {\ttfamily H} and {\ttfamily W}, one can pass {\ttfamily N\+C\+D\+H\+Wc} as well. \\
\hline
{\em count\+\_\+include\+\_\+pad} & Whether include padding in the calculation when pool\+\_\+type is \textquotesingle{}avg\textquotesingle{}\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in the same layout 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!pool\+\_\+grad@{pool\+\_\+grad}}
\index{pool\+\_\+grad@{pool\+\_\+grad}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{pool\+\_\+grad(const Tensor \&out\+\_\+grad, const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&kernel\+\_\+size, const Array$<$ Prim\+Expr $>$ \&stride\+\_\+size, const Array$<$ Prim\+Expr $>$ \&padding\+\_\+size, Pool\+Type pool\+\_\+type, bool ceil\+\_\+mode, const std\+::string \&layout=""N\+C\+HW"", bool count\+\_\+include\+\_\+pad=true)}{pool_grad(const Tensor &out_grad, const Tensor &x, const Array< PrimExpr > &kernel_size, const Array< PrimExpr > &stride_size, const Array< PrimExpr > &padding_size, PoolType pool_type, bool ceil_mode, const std::string &layout="NCHW", bool count_include_pad=true)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::pool\+\_\+grad (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{out\+\_\+grad, }
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{kernel\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{stride\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{padding\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{bool}]{ceil\+\_\+mode, }
\item[{const std\+::string \&}]{layout = {\ttfamily \char`\"{}NCHW\char`\"{}}, }
\item[{bool}]{count\+\_\+include\+\_\+pad = {\ttfamily true}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a4f915567f195ade4a17743a5e7654e88}{}\label{namespacetopi_1_1nn_a4f915567f195ade4a17743a5e7654e88}


Calculate gradient of pooling on height and width dimension of data. It decides the height and width dimension according to the layout string, in which \textquotesingle{}W\textquotesingle{} and \textquotesingle{}H\textquotesingle{} means width and height respectively. Width and height dimension cannot be split. For example, N\+C\+HW, N\+C\+H\+W16c, etc. are valid for pool, while N\+C\+H\+W16w, N\+C\+H\+W16h are not. See {\itshape layout} for more information of the layout string convention. 


\begin{DoxyParams}{Parameters}
{\em out\+\_\+grad} & The output gradient tensor. \\
\hline
{\em x} & The input tensor. \\
\hline
{\em kernel\+\_\+size} & Vector of two ints\+: \{kernel\+\_\+height, kernel\+\_\+width\} \\
\hline
{\em stride\+\_\+size} & Vector of two ints\+: \{stride\+\_\+height, stride\+\_\+width\} \\
\hline
{\em padding\+\_\+size} & Vector of two ints\+: \{padding\+\_\+height, padding\+\_\+width\} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em ceil\+\_\+mode} & Whether to use ceil when calculating the output size \\
\hline
{\em layout} & The input layout. Pooling supports any layout as long as \textquotesingle{}H\textquotesingle{} and \textquotesingle{}W\textquotesingle{} appear. The layout is supposed to be composed of upper cases, lower cases and (optional) numbers, where upper case indicates a dimension and the corresponding lower case (with factor size) indicates the split dimension. For example, N\+C\+H\+W16c can describe a 5-\/D tensor of \mbox{[}batch\+\_\+size, channel, height, width, channel\+\_\+block\mbox{]}. (in which factor size {\ttfamily 16} will not be used in pooling but for other operators, it can be used to decide the output shape). Since pooling does not care about the factor size of dimensions other than {\ttfamily H} and {\ttfamily W}, one can pass {\ttfamily N\+C\+H\+Wc} as well. \\
\hline
{\em count\+\_\+include\+\_\+pad} & Whether include padding in the calculation when pool\+\_\+type is \textquotesingle{}avg\textquotesingle{}\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in the same layout 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!pool\+\_\+grad\+\_\+impl@{pool\+\_\+grad\+\_\+impl}}
\index{pool\+\_\+grad\+\_\+impl@{pool\+\_\+grad\+\_\+impl}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{pool\+\_\+grad\+\_\+impl(const Tensor \&out\+\_\+grad, const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&kernel\+\_\+size, const Array$<$ Prim\+Expr $>$ \&stride\+\_\+size, const Array$<$ Prim\+Expr $>$ \&padding\+\_\+size, Pool\+Type pool\+\_\+type, bool ceil\+\_\+mode, const size\+\_\+t height\+\_\+axis, const size\+\_\+t width\+\_\+axis, bool count\+\_\+include\+\_\+pad)}{pool_grad_impl(const Tensor &out_grad, const Tensor &x, const Array< PrimExpr > &kernel_size, const Array< PrimExpr > &stride_size, const Array< PrimExpr > &padding_size, PoolType pool_type, bool ceil_mode, const size_t height_axis, const size_t width_axis, bool count_include_pad)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::pool\+\_\+grad\+\_\+impl (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{out\+\_\+grad, }
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{kernel\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{stride\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{padding\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{bool}]{ceil\+\_\+mode, }
\item[{const size\+\_\+t}]{height\+\_\+axis, }
\item[{const size\+\_\+t}]{width\+\_\+axis, }
\item[{bool}]{count\+\_\+include\+\_\+pad}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_af2c25e8b3ab3cac1c2896cb750838337}{}\label{namespacetopi_1_1nn_af2c25e8b3ab3cac1c2896cb750838337}
\index{topi\+::nn@{topi\+::nn}!pool\+\_\+impl@{pool\+\_\+impl}}
\index{pool\+\_\+impl@{pool\+\_\+impl}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{pool\+\_\+impl(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&kernel\+\_\+size, const Array$<$ Prim\+Expr $>$ \&stride\+\_\+size, const Array$<$ Prim\+Expr $>$ \&padding\+\_\+size, Pool\+Type pool\+\_\+type, bool ceil\+\_\+mode, const size\+\_\+t height\+\_\+axis, const size\+\_\+t width\+\_\+axis, bool count\+\_\+include\+\_\+pad)}{pool_impl(const Tensor &x, const Array< PrimExpr > &kernel_size, const Array< PrimExpr > &stride_size, const Array< PrimExpr > &padding_size, PoolType pool_type, bool ceil_mode, const size_t height_axis, const size_t width_axis, bool count_include_pad)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::pool\+\_\+impl (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{kernel\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{stride\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{padding\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{bool}]{ceil\+\_\+mode, }
\item[{const size\+\_\+t}]{height\+\_\+axis, }
\item[{const size\+\_\+t}]{width\+\_\+axis, }
\item[{bool}]{count\+\_\+include\+\_\+pad}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ad51533b09956d7bc8de2537adf3b6b77}{}\label{namespacetopi_1_1nn_ad51533b09956d7bc8de2537adf3b6b77}


Perform pooling on height and width dimension of data. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor \\
\hline
{\em kernel\+\_\+size} & Vector of two ints\+: \{kernel\+\_\+height, kernel\+\_\+width\} \\
\hline
{\em stride\+\_\+size} & Vector of two ints\+: \{stride\+\_\+height, stride\+\_\+width\} \\
\hline
{\em padding\+\_\+size} & Vector of two ints\+: \{padding\+\_\+height, padding\+\_\+width\} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em ceil\+\_\+mode} & Whether to use ceil when calculating the output size \\
\hline
{\em height\+\_\+axis} & index of the height dimension \\
\hline
{\em width\+\_\+axis} & index of the width dimension \\
\hline
{\em count\+\_\+include\+\_\+pad} & Whether include padding in the calculation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in same layout order 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!pool\+\_\+impl\+\_\+nd@{pool\+\_\+impl\+\_\+nd}}
\index{pool\+\_\+impl\+\_\+nd@{pool\+\_\+impl\+\_\+nd}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{pool\+\_\+impl\+\_\+nd(const Tensor \&x, const Array$<$ Prim\+Expr $>$ \&kernel\+\_\+size, const Array$<$ Prim\+Expr $>$ \&stride\+\_\+size, const Array$<$ Prim\+Expr $>$ \&padding\+\_\+size, Pool\+Type pool\+\_\+type, bool ceil\+\_\+mode, const std\+::vector$<$ int $>$ \&axis, bool count\+\_\+include\+\_\+pad)}{pool_impl_nd(const Tensor &x, const Array< PrimExpr > &kernel_size, const Array< PrimExpr > &stride_size, const Array< PrimExpr > &padding_size, PoolType pool_type, bool ceil_mode, const std::vector< int > &axis, bool count_include_pad)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::pool\+\_\+impl\+\_\+nd (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{kernel\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{stride\+\_\+size, }
\item[{const {\bf Array}$<$ {\bf Prim\+Expr} $>$ \&}]{padding\+\_\+size, }
\item[{{\bf Pool\+Type}}]{pool\+\_\+type, }
\item[{bool}]{ceil\+\_\+mode, }
\item[{const std\+::vector$<$ int $>$ \&}]{axis, }
\item[{bool}]{count\+\_\+include\+\_\+pad}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a0b2681e29b1f733835ffe2e6b3b69c13}{}\label{namespacetopi_1_1nn_a0b2681e29b1f733835ffe2e6b3b69c13}


Perform pooling on N-\/dimension of data. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor \\
\hline
{\em kernel\+\_\+size} & Vector of N ints \\
\hline
{\em stride\+\_\+size} & Vector of N ints \\
\hline
{\em padding\+\_\+size} & Vector of N$\ast$2 ints \mbox{[}head\+\_\+pad\+\_\+d1, head\+\_\+pad\+\_\+d2, ..., head\+\_\+pad\+\_\+dN, tail\+\_\+pad\+\_\+d1, tail\+\_\+pad\+\_\+d2, ..., tail\+\_\+pad\+\_\+dN\mbox{]} \\
\hline
{\em pool\+\_\+type} & The type of pooling operator \\
\hline
{\em ceil\+\_\+mode} & Whether to use ceil when calculating the output size \\
\hline
{\em axis} & Vector of indices for the N dimensions \\
\hline
{\em count\+\_\+include\+\_\+pad} & Whether include padding in the calculation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
The output tensor in same layout order 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!scale\+\_\+shift\+\_\+nchw@{scale\+\_\+shift\+\_\+nchw}}
\index{scale\+\_\+shift\+\_\+nchw@{scale\+\_\+shift\+\_\+nchw}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{scale\+\_\+shift\+\_\+nchw(const Tensor \&x, const Tensor \&scale, const Tensor \&shift, std\+::string name=""Scale\+Shift"", std\+::string tag=k\+Broadcast)}{scale_shift_nchw(const Tensor &x, const Tensor &scale, const Tensor &shift, std::string name="ScaleShift", std::string tag=kBroadcast)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::scale\+\_\+shift\+\_\+nchw (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Tensor} \&}]{scale, }
\item[{const {\bf Tensor} \&}]{shift, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}ScaleShift\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily {\bf k\+Broadcast}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ae9516b844130a2189ad2989b646ec2b1}{}\label{namespacetopi_1_1nn_ae9516b844130a2189ad2989b646ec2b1}


Scale and shift with N\+C\+HW order. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. \\
\hline
{\em scale} & Scale tensor, 1-\/D of size channel \\
\hline
{\em shift} & Shift tensor, 1-\/D of size channel \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A Tensor whose op member is the scale shift operation 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!scale\+\_\+shift\+\_\+nhwc@{scale\+\_\+shift\+\_\+nhwc}}
\index{scale\+\_\+shift\+\_\+nhwc@{scale\+\_\+shift\+\_\+nhwc}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{scale\+\_\+shift\+\_\+nhwc(const Tensor \&x, const Tensor \&scale, const Tensor \&shift, std\+::string name=""Scale\+Shift"", std\+::string tag=k\+Broadcast)}{scale_shift_nhwc(const Tensor &x, const Tensor &scale, const Tensor &shift, std::string name="ScaleShift", std::string tag=kBroadcast)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::scale\+\_\+shift\+\_\+nhwc (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{const {\bf Tensor} \&}]{scale, }
\item[{const {\bf Tensor} \&}]{shift, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}ScaleShift\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily {\bf k\+Broadcast}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_ab19179f1b4a73a42b17ddf2f5fea46fc}{}\label{namespacetopi_1_1nn_ab19179f1b4a73a42b17ddf2f5fea46fc}


Scale and shift with N\+H\+WC order. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. \\
\hline
{\em scale} & Scale tensor, 1-\/D of size channel \\
\hline
{\em shift} & Shift tensor, 1-\/D of size channel \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A Tensor whose op member is the scale shift operation 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!softmax@{softmax}}
\index{softmax@{softmax}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{softmax(const Tensor \&x, int axis=-\/1, std\+::string name=""tensor"", std\+::string tag=""softmax\+\_\+output"")}{softmax(const Tensor &x, int axis=-1, std::string name="tensor", std::string tag="softmax_output")}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Tensor} topi\+::nn\+::softmax (
\begin{DoxyParamCaption}
\item[{const {\bf Tensor} \&}]{x, }
\item[{int}]{axis = {\ttfamily -\/1}, }
\item[{std\+::string}]{name = {\ttfamily \char`\"{}tensor\char`\"{}}, }
\item[{std\+::string}]{tag = {\ttfamily \char`\"{}softmax\+\_\+output\char`\"{}}}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a1c845f2e02a677c556929d41a399e729}{}\label{namespacetopi_1_1nn_a1c845f2e02a677c556929d41a399e729}


Softmax activation. 


\begin{DoxyParams}{Parameters}
{\em x} & The input tensor. Can be any dimension \\
\hline
{\em axis} & The channel axis along which softmax is performed \\
\hline
{\em name} & The name of the operation \\
\hline
{\em tag} & The tag to mark the operation\\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
A Tensor whose op member is the softmax operation 
\end{DoxyReturn}
\index{topi\+::nn@{topi\+::nn}!start\+\_\+index@{start\+\_\+index}}
\index{start\+\_\+index@{start\+\_\+index}!topi\+::nn@{topi\+::nn}}
\subsubsection[{\texorpdfstring{start\+\_\+index(const Var \&out\+\_\+index, const Prim\+Expr \&odim, const Prim\+Expr \&idim)}{start_index(const Var &out_index, const PrimExpr &odim, const PrimExpr &idim)}}]{\setlength{\rightskip}{0pt plus 5cm}{\bf Prim\+Expr} topi\+::nn\+::start\+\_\+index (
\begin{DoxyParamCaption}
\item[{const {\bf Var} \&}]{out\+\_\+index, }
\item[{const {\bf Prim\+Expr} \&}]{odim, }
\item[{const {\bf Prim\+Expr} \&}]{idim}
\end{DoxyParamCaption}
)\hspace{0.3cm}{\ttfamily [inline]}}\hypertarget{namespacetopi_1_1nn_a91b52c68356d23123474ebf10f9b0140}{}\label{namespacetopi_1_1nn_a91b52c68356d23123474ebf10f9b0140}
